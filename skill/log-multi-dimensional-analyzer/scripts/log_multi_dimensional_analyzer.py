#!/usr/bin/env python3
"""
æ—¥å¿—å¤šç»´åº¦åˆ†æå·¥å…·

æ”¯æŒé€šè¿‡ bklog-bkop MCP å·¥å…·è¿›è¡Œæ—¥å¿—æ•°æ®çš„å¤šç»´åº¦åˆ†æï¼š
- æ”¯æŒè‡ªå®šä¹‰è¿‡æ»¤æ¡ä»¶ï¼ˆå¦‚ namespaceã€svr ç­‰ï¼‰
- æ”¯æŒæŒ‰æŒ‡å®šå­—æ®µåˆ†ç»„ï¼ˆå¦‚ file_nameï¼‰
- æ”¯æŒæŒ‰æŒ‡å®šç»´åº¦æ‹†åˆ†ï¼ˆå¦‚æ—¥å¿—çº§åˆ« levelï¼‰
- ç”Ÿæˆå¤šç»´åº¦ç»Ÿè®¡ç»“æœ
- è‡ªåŠ¨è¯†åˆ«æ—¥å¿—åˆ†æç»“æœä¸­çš„èµ„æºï¼ˆpodã€namespaceã€serviceã€host ç­‰ï¼‰
- é€šè¿‡ bkmonitor-metrics-bkop MCP å·¥å…·æŸ¥è¯¢ç›¸å…³ç›‘æ§æŒ‡æ ‡
- é€šè¿‡ mcp-server-chart MCP å·¥å…·ç”ŸæˆæŒ‡æ ‡å¯è§†åŒ–å›¾è¡¨
- åœ¨åˆ†ææŠ¥å‘Šä¸­åµŒå…¥æŒ‡æ ‡å›¾ç‰‡å’Œç»Ÿè®¡ä¿¡æ¯

æ‰§è¡Œæµç¨‹ï¼š
1. æ—¥å¿—åˆ†æï¼ˆä½¿ç”¨ bkmonitor-log-bkopï¼‰
2. æŒ‡æ ‡è·å–ï¼ˆä½¿ç”¨ bkmonitor-metrics-bkopï¼‰
3. æŠ˜çº¿å›¾ç»˜åˆ¶ï¼ˆä½¿ç”¨ mcp-server-chartï¼‰
"""

import json
import sys
import time
import os
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict
from datetime import datetime
from pathlib import Path


class LogMultiDimensionalAnalyzer:
    """æ—¥å¿—å¤šç»´åº¦åˆ†æå™¨"""
    
    def __init__(self, mcp_client=None, enable_metrics_query: bool = True, metrics_output_dir: str = "metrics"):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            mcp_client: MCP å®¢æˆ·ç«¯å®ä¾‹ï¼Œå¦‚æœä¸º None åˆ™éœ€è¦åœ¨è°ƒç”¨æ—¶æ‰‹åŠ¨ä¼ å…¥ç»“æœ
            enable_metrics_query: æ˜¯å¦å¯ç”¨æŒ‡æ ‡æŸ¥è¯¢åŠŸèƒ½ï¼Œé»˜è®¤ True
            metrics_output_dir: æŒ‡æ ‡å›¾ç‰‡è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ "metrics"
        """
        self.mcp_client = mcp_client
        self.enable_metrics_query = enable_metrics_query
        self.metrics_output_dir = Path(metrics_output_dir)
        self.metrics_output_dir.mkdir(parents=True, exist_ok=True)
        
        # èµ„æºè¯†åˆ«æ˜ å°„ï¼šå­—æ®µå -> èµ„æºç±»å‹
        self.resource_field_mapping = {
            "pod": "pod",
            "pod_name": "pod",
            "namespace": "namespace",
            "service": "service",
            "svc": "service",
            "host": "host",
            "serverIp": "host",
            "ip": "host",
            "container": "container",
            "container_name": "container"
        }
        
        # èµ„æºç±»å‹å¯¹åº”çš„æŒ‡æ ‡æŸ¥è¯¢é…ç½®
        self.resource_metrics_config = {
            "pod": {
                "cpu": "avg(avg_over_time(bkmonitor:system:cpu_summary:usage{pod=\"%s\"}[1m]))",
                "memory": "avg(avg_over_time(bkmonitor:system:mem:pct_used{pod=\"%s\"}[1m]))"
            },
            "namespace": {
                "cpu": "avg(avg_over_time(bkmonitor:system:cpu_summary:usage{namespace=\"%s\"}[1m]))",
                "memory": "avg(avg_over_time(bkmonitor:system:mem:pct_used{namespace=\"%s\"}[1m]))"
            },
            "host": {
                "cpu": "avg(avg_over_time(bkmonitor:system:cpu_summary:usage{ip=\"%s\"}[1m]))",
                "memory": "avg(avg_over_time(bkmonitor:system:mem:pct_used{ip=\"%s\"}[1m]))",
                "disk": "avg(avg_over_time(bkmonitor:system:disk:in_use{ip=\"%s\"}[1m]))"
            },
            "service": {
                "cpu": "avg(avg_over_time(bkmonitor:system:cpu_summary:usage{service=\"%s\"}[1m]))",
                "memory": "avg(avg_over_time(bkmonitor:system:mem:pct_used{service=\"%s\"}[1m]))"
            }
        }
    
    def analyze_multi_dimensional(
        self,
        bk_biz_id: str,
        index_set_id: str,
        filter_fields: Dict[str, Any],
        group_by_field: str,
        split_by_field: str,
        start_time: Optional[int] = None,
        end_time: Optional[int] = None,
        query_string: str = "*",
        limit: int = 500
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå¤šç»´åº¦åˆ†æ
        
        Args:
            bk_biz_id: ä¸šåŠ¡ID
            index_set_id: ç´¢å¼•é›†ID
            filter_fields: è¿‡æ»¤å­—æ®µå­—å…¸ï¼Œå¦‚ {"namespace": "xxx", "svr": "yyy"}
            group_by_field: åˆ†ç»„å­—æ®µï¼Œå¦‚ "file_name"
            split_by_field: æ‹†åˆ†ç»´åº¦å­—æ®µï¼Œå¦‚ "level"
            start_time: å¼€å§‹æ—¶é—´æˆ³ï¼ˆç§’ï¼‰ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨å½“å‰æ—¶é—´å‰1å°æ—¶
            end_time: ç»“æŸæ—¶é—´æˆ³ï¼ˆç§’ï¼‰ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨å½“å‰æ—¶é—´
            query_string: æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œé»˜è®¤ä¸º "*"
            limit: æŸ¥è¯¢é™åˆ¶ï¼Œé»˜è®¤ 500
            
        Returns:
            å¤šç»´åº¦åˆ†æç»“æœå­—å…¸
        """
        # è®¡ç®—æ—¶é—´èŒƒå›´
        if end_time is None:
            end_time = int(time.time())
        if start_time is None:
            start_time = end_time - 3600  # é»˜è®¤æœ€è¿‘1å°æ—¶
        
        # æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²
        query_parts = [query_string]
        for field, value in filter_fields.items():
            if isinstance(value, list):
                query_parts.append(f"({field}:({' OR '.join(value)}))")
            else:
                query_parts.append(f"{field}:{value}")
        
        final_query = " AND ".join([q for q in query_parts if q != "*" or len(query_parts) == 1])
        
        # ç¬¬ä¸€æ­¥ï¼šè·å–ç´¢å¼•é›†å­—æ®µä¿¡æ¯ï¼ˆç”¨äºéªŒè¯å­—æ®µæ˜¯å¦å­˜åœ¨ï¼‰
        if self.mcp_client:
            try:
                fields_result = self.mcp_client.call_tool(
                    "bkmonitor-log-bkop",
                    "get_index_set_fields",
                    {
                        "query_param": {
                            "bk_biz_id": bk_biz_id,
                            "index_set_id": index_set_id
                        }
                    }
                )
                # éªŒè¯å­—æ®µæ˜¯å¦å­˜åœ¨
                available_fields = [f.get("field_name", "") for f in fields_result.get("fields", [])]
                if group_by_field not in available_fields:
                    print(f"è­¦å‘Š: åˆ†ç»„å­—æ®µ '{group_by_field}' å¯èƒ½ä¸å­˜åœ¨äºç´¢å¼•é›†ä¸­")
                if split_by_field not in available_fields:
                    print(f"è­¦å‘Š: æ‹†åˆ†å­—æ®µ '{split_by_field}' å¯èƒ½ä¸å­˜åœ¨äºç´¢å¼•é›†ä¸­")
            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•è·å–å­—æ®µä¿¡æ¯: {e}")
        
        # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨ analyze_field è¿›è¡Œå­—æ®µåˆ†æ
        # å…ˆåˆ†æ group_by_fieldï¼Œè·å–æ‰€æœ‰åˆ†ç»„å€¼
        group_by_values = self._get_field_values(
            bk_biz_id, index_set_id, group_by_field,
            final_query, start_time, end_time
        )
        
        # ç¬¬ä¸‰æ­¥ï¼šå¯¹æ¯ä¸ªåˆ†ç»„å€¼ï¼Œåˆ†æ split_by_field
        results = {}
        for group_value in group_by_values:
            # æ„å»ºåŒ…å«åˆ†ç»„å€¼çš„æŸ¥è¯¢
            group_query = f"{final_query} AND {group_by_field}:{group_value}"
            
            # åˆ†ææ‹†åˆ†å­—æ®µ
            split_results = self._analyze_split_field(
                bk_biz_id, index_set_id, split_by_field,
                group_query, start_time, end_time
            )
            
            results[group_value] = split_results
        
        analysis_result = {
            "analysis_config": {
                "bk_biz_id": bk_biz_id,
                "index_set_id": index_set_id,
                "filter_fields": filter_fields,
                "group_by_field": group_by_field,
                "split_by_field": split_by_field,
                "time_range": {
                    "start_time": start_time,
                    "end_time": end_time,
                    "start_time_str": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(start_time)),
                    "end_time_str": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(end_time))
                },
                "query_string": final_query
            },
            "results": results,
            "summary": self._generate_summary(results)
        }
        
        # å¦‚æœå¯ç”¨æŒ‡æ ‡æŸ¥è¯¢ï¼Œè¯†åˆ«èµ„æºå¹¶æŸ¥è¯¢æŒ‡æ ‡
        if self.enable_metrics_query and self.mcp_client:
            metrics_results = self._query_related_metrics(
                bk_biz_id, filter_fields, results, start_time, end_time
            )
            analysis_result["metrics"] = metrics_results
        
        return analysis_result
    
    def _get_field_values(
        self,
        bk_biz_id: str,
        index_set_id: str,
        field_name: str,
        query_string: str,
        start_time: int,
        end_time: int
    ) -> List[str]:
        """è·å–å­—æ®µçš„æ‰€æœ‰å”¯ä¸€å€¼"""
        if not self.mcp_client:
            return []
        
        try:
            result = self.mcp_client.call_tool(
                "bkmonitor-log-bkop",
                "analyze_field",
                {
                    "body_param": {
                        "bk_biz_id": bk_biz_id,
                        "index_set_id": index_set_id,
                        "field_name": field_name,
                        "query_string": query_string,
                        "start_time": str(start_time),
                        "end_time": str(end_time),
                        "group_by": "true",
                        "order_by": "value",
                        "limit": "100"  # è·å– Top 100 çš„åˆ†ç»„å€¼
                    }
                }
            )
            
            # æå–å­—æ®µå€¼
            values = []
            if "data" in result and "list" in result["data"]:
                for item in result["data"]["list"]:
                    if "name" in item:
                        values.append(str(item["name"]))
                    elif "key" in item:
                        values.append(str(item["key"]))
            
            return values
        except Exception as e:
            print(f"é”™è¯¯: è·å–å­—æ®µå€¼å¤±è´¥: {e}")
            return []
    
    def _analyze_split_field(
        self,
        bk_biz_id: str,
        index_set_id: str,
        field_name: str,
        query_string: str,
        start_time: int,
        end_time: int
    ) -> Dict[str, Any]:
        """åˆ†ææ‹†åˆ†å­—æ®µçš„åˆ†å¸ƒ"""
        if not self.mcp_client:
            return {}
        
        try:
            result = self.mcp_client.call_tool(
                "bkmonitor-log-bkop",
                "analyze_field",
                {
                    "body_param": {
                        "bk_biz_id": bk_biz_id,
                        "index_set_id": index_set_id,
                        "field_name": field_name,
                        "query_string": query_string,
                        "start_time": str(start_time),
                        "end_time": str(end_time),
                        "group_by": "true",
                        "order_by": "value",
                        "limit": "50"  # è·å– Top 50 çš„æ‹†åˆ†å€¼
                    }
                }
            )
            
            # ç»„ç»‡ç»“æœ
            split_data = {}
            total_count = 0
            
            if "data" in result and "list" in result["data"]:
                for item in result["data"]["list"]:
                    key = str(item.get("name") or item.get("key", ""))
                    value = item.get("value", 0)
                    split_data[key] = value
                    total_count += value
            
            return {
                "distribution": split_data,
                "total": total_count,
                "count": len(split_data)
            }
        except Exception as e:
            print(f"é”™è¯¯: åˆ†ææ‹†åˆ†å­—æ®µå¤±è´¥: {e}")
            return {}
    
    def _generate_summary(self, results: Dict[str, Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡"""
        summary = {
            "total_groups": len(results),
            "group_totals": {},
            "split_field_summary": defaultdict(int),
            "overall_total": 0
        }
        
        for group_value, group_data in results.items():
            if "total" in group_data:
                group_total = group_data["total"]
                summary["group_totals"][group_value] = group_total
                summary["overall_total"] += group_total
                
                # æ±‡æ€»æ‹†åˆ†å­—æ®µçš„åˆ†å¸ƒ
                if "distribution" in group_data:
                    for split_value, count in group_data["distribution"].items():
                        summary["split_field_summary"][split_value] += count
        
        return summary
    
    def _identify_resources(self, filter_fields: Dict[str, Any], results: Dict[str, Dict]) -> List[Dict[str, str]]:
        """
        ä»åˆ†æç»“æœä¸­è¯†åˆ«èµ„æºï¼ˆå¢å¼ºç‰ˆï¼‰
        
        Args:
            filter_fields: è¿‡æ»¤å­—æ®µå­—å…¸
            results: åˆ†æç»“æœ
            
        Returns:
            èµ„æºåˆ—è¡¨ï¼Œæ¯ä¸ªèµ„æºåŒ…å« type å’Œ value
        """
        resources = []
        seen_resources = set()  # é¿å…é‡å¤
        
        # ä»è¿‡æ»¤å­—æ®µä¸­è¯†åˆ«èµ„æºï¼ˆæ”¯æŒå¤§å°å†™ä¸æ•æ„ŸåŒ¹é…ï¼‰
        for field, value in filter_fields.items():
            # å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
            resource_type = self.resource_field_mapping.get(field)
            # å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•å¤§å°å†™ä¸æ•æ„ŸåŒ¹é…
            if not resource_type:
                field_lower = field.lower()
                # åˆ›å»ºå¤§å°å†™ä¸æ•æ„Ÿçš„æ˜ å°„
                case_insensitive_mapping = {k.lower(): v for k, v in self.resource_field_mapping.items()}
                resource_type = case_insensitive_mapping.get(field_lower)
            
            if resource_type:
                print(f"ğŸ” è¯†åˆ«åˆ°èµ„æº: {resource_type} = {value} (å­—æ®µ: {field})")
                if isinstance(value, list):
                    for v in value:
                        resource_key = f"{resource_type}:{v}"
                        if resource_key not in seen_resources:
                            resources.append({"type": resource_type, "field": field, "value": str(v)})
                            seen_resources.add(resource_key)
                else:
                    resource_key = f"{resource_type}:{value}"
                    if resource_key not in seen_resources:
                        resources.append({"type": resource_type, "field": field, "value": str(value)})
                        seen_resources.add(resource_key)
        
        # ä»ç»“æœä¸­è¯†åˆ«èµ„æºï¼ˆå¦‚æœåˆ†ç»„å­—æ®µæ˜¯èµ„æºå­—æ®µï¼‰
        # ä¾‹å¦‚ï¼šå¦‚æœæŒ‰ serverIp åˆ†ç»„ï¼Œå¯ä»¥ä»åˆ†ç»„å€¼ä¸­è¯†åˆ«ä¸»æœºèµ„æº
        for group_value, group_data in results.items():
            # æ£€æŸ¥åˆ†ç»„å€¼æ˜¯å¦å¯èƒ½æ˜¯èµ„æºæ ‡è¯†ç¬¦
            # ä¾‹å¦‚ï¼šIPåœ°å€æ ¼å¼ã€Podåç§°æ ¼å¼ç­‰
            if self._is_ip_address(group_value):
                resource_key = f"host:{group_value}"
                if resource_key not in seen_resources:
                    resources.append({"type": "host", "field": "group_value", "value": str(group_value)})
                    seen_resources.add(resource_key)
        
        return resources
    
    def _is_ip_address(self, value: str) -> bool:
        """åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦æ˜¯IPåœ°å€"""
        import re
        ip_pattern = r'^(\d{1,3}\.){3}\d{1,3}$'
        return bool(re.match(ip_pattern, str(value)))
    
    def _query_related_metrics(
        self,
        bk_biz_id: str,
        filter_fields: Dict[str, Any],
        results: Dict[str, Dict],
        start_time: int,
        end_time: int
    ) -> Dict[str, Any]:
        """
        æŸ¥è¯¢ç›¸å…³æŒ‡æ ‡
        
        Args:
            bk_biz_id: ä¸šåŠ¡ID
            filter_fields: è¿‡æ»¤å­—æ®µ
            results: åˆ†æç»“æœ
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            
        Returns:
            æŒ‡æ ‡æŸ¥è¯¢ç»“æœå­—å…¸
        """
        if not self.mcp_client:
            return {}
        
        metrics_results = {
            "resources": [],
            "queries": [],
            "charts": []
        }
        
        # è¯†åˆ«èµ„æº
        resources = self._identify_resources(filter_fields, results)
        
        for resource in resources:
            resource_type = resource["type"]
            resource_value = resource["value"]
            
            # è·å–è¯¥èµ„æºç±»å‹çš„æŒ‡æ ‡é…ç½®
            metrics_config = self.resource_metrics_config.get(resource_type, {})
            
            resource_metrics = {
                "resource_type": resource_type,
                "resource_value": resource_value,
                "resource_field": resource["field"],
                "metrics": {}
            }
            
            # æŸ¥è¯¢æ¯ä¸ªæŒ‡æ ‡
            for metric_name, promql_template in metrics_config.items():
                try:
                    # æ„å»º PromQL æŸ¥è¯¢
                    promql = promql_template % resource_value
                    
                    # æ‰§è¡ŒæŸ¥è¯¢
                    query_result = self.mcp_client.call_tool(
                        "bkmonitor-metrics-bkop",
                        "execute_range_query",
                        {
                            "body_param": {
                                "bk_biz_id": bk_biz_id,
                                "promql": promql,
                                "start_time": str(start_time),
                                "end_time": str(end_time),
                                "step": "1m"
                            }
                        }
                    )
                    
                    # è§£ææŸ¥è¯¢ç»“æœ
                    metric_data = self._parse_metric_result(query_result)
                    
                    if metric_data:
                        resource_metrics["metrics"][metric_name] = metric_data
                        
                        # ç”Ÿæˆå›¾è¡¨
                        chart_path = self._generate_metric_chart(
                            metric_data,
                            f"{resource_type}_{resource_value}_{metric_name}",
                            f"{resource_type}_{resource_value}_{metric_name}",
                            start_time,
                            end_time
                        )
                        
                        if chart_path:
                            resource_metrics["metrics"][metric_name]["chart_path"] = chart_path
                            metrics_results["charts"].append(chart_path)
                    
                    metrics_results["queries"].append({
                        "resource": resource_value,
                        "metric": metric_name,
                        "promql": promql,
                        "success": metric_data is not None
                    })
                    
                except Exception as e:
                    print(f"è­¦å‘Š: æŸ¥è¯¢æŒ‡æ ‡å¤±è´¥ {resource_type}={resource_value}, metric={metric_name}: {e}")
                    metrics_results["queries"].append({
                        "resource": resource_value,
                        "metric": metric_name,
                        "promql": promql_template % resource_value,
                        "success": False,
                        "error": str(e)
                    })
            
            if resource_metrics["metrics"]:
                metrics_results["resources"].append(resource_metrics)
        
        return metrics_results
    
    def _parse_metric_result(self, query_result: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        è§£ææŒ‡æ ‡æŸ¥è¯¢ç»“æœ
        
        Args:
            query_result: MCP å·¥å…·è¿”å›çš„æŸ¥è¯¢ç»“æœ
            
        Returns:
            è§£æåçš„æŒ‡æ ‡æ•°æ®ï¼ŒåŒ…å« timestamps å’Œ values åˆ—è¡¨
        """
        try:
            # æ ¹æ®å®é™…è¿”å›æ ¼å¼è§£æ
            # bkmonitor-metrics-bkop è¿”å›æ ¼å¼: {"data": {"series": [{"datapoints": [[timestamp, value], ...]}]}}
            if "data" not in query_result:
                return None
            
            data = query_result["data"]
            timestamps = []
            values = []
            
            # å¤„ç† bkmonitor-metrics-bkop çš„å®é™…è¿”å›æ ¼å¼
            if isinstance(data, dict):
                # æ ¼å¼: {"series": [{"datapoints": [[timestamp_ms, value], ...]}]}
                series = data.get("series", [])
                for series_item in series:
                    datapoints = series_item.get("datapoints", [])
                    for point in datapoints:
                        if isinstance(point, list) and len(point) >= 2:
                            # point[0] æ˜¯æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰ï¼Œpoint[1] æ˜¯å€¼
                            timestamp_ms = point[0]
                            value = point[1]
                            if value is not None:  # è¿‡æ»¤æ‰ None å€¼
                                timestamps.append(timestamp_ms)
                                values.append(float(value))
            
            # å…¼å®¹å…¶ä»–å¯èƒ½çš„æ ¼å¼
            elif isinstance(data, list):
                for item in data:
                    if isinstance(item, dict):
                        # å¯èƒ½æ˜¯ {"timestamp": xxx, "value": xxx} æ ¼å¼
                        if "timestamp" in item and "value" in item:
                            timestamps.append(item["timestamp"])
                            values.append(float(item["value"]))
                        # å¯èƒ½æ˜¯ {"time": xxx, "value": xxx} æ ¼å¼
                        elif "time" in item and "value" in item:
                            timestamps.append(item["time"])
                            values.append(float(item["value"]))
            
            if not timestamps or not values:
                return None
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            if values:
                avg_value = sum(values) / len(values)
                max_value = max(values)
                min_value = min(values)
            else:
                avg_value = max_value = min_value = 0
            
            return {
                "timestamps": timestamps,
                "values": values,
                "statistics": {
                    "avg": avg_value,
                    "max": max_value,
                    "min": min_value,
                    "count": len(values)
                }
            }
        except Exception as e:
            print(f"é”™è¯¯: è§£ææŒ‡æ ‡ç»“æœå¤±è´¥: {e}")
            return None
    
    def _generate_metric_chart(
        self,
        metric_data: Dict[str, Any],
        resource_identifier: str,
        metric_name: str,
        start_time: int,
        end_time: int
    ) -> Optional[str]:
        """
        ç”ŸæˆæŒ‡æ ‡å›¾è¡¨ï¼ˆä½¿ç”¨ chart MCP å·¥å…·ï¼‰
        
        Args:
            metric_data: æŒ‡æ ‡æ•°æ®ï¼ŒåŒ…å« timestamps å’Œ values
            resource_identifier: èµ„æºæ ‡è¯†ç¬¦ï¼ˆç”¨äºæ–‡ä»¶åå’Œæ ‡é¢˜ï¼‰
            metric_name: æŒ‡æ ‡åç§°
            start_time: å¼€å§‹æ—¶é—´ï¼ˆUnixæ—¶é—´æˆ³ï¼Œç§’ï¼‰
            end_time: ç»“æŸæ—¶é—´ï¼ˆUnixæ—¶é—´æˆ³ï¼Œç§’ï¼‰
            
        Returns:
            å›¾è¡¨æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœç”Ÿæˆå¤±è´¥è¿”å› None
        """
        if not self.mcp_client:
            print("è­¦å‘Š: MCP å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œæ— æ³•ç”Ÿæˆå›¾è¡¨")
            return None
        
        try:
            timestamps = metric_data.get("timestamps", [])
            values = metric_data.get("values", [])
            
            if not timestamps or not values:
                return None
            
            # è½¬æ¢æ—¶é—´æˆ³ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼ˆchart MCP éœ€è¦ time å­—æ®µä¸ºå­—ç¬¦ä¸²ï¼‰
            # å¤„ç†æ¯«ç§’æ—¶é—´æˆ³ï¼ˆbkmonitorè¿”å›çš„æ˜¯æ¯«ç§’ï¼‰
            time_strings = []
            for ts in timestamps:
                if isinstance(ts, (int, float)):
                    if ts > 1e10:  # æ¯«ç§’æ—¶é—´æˆ³
                        dt = datetime.fromtimestamp(ts / 1000)
                    else:  # ç§’æ—¶é—´æˆ³
                        dt = datetime.fromtimestamp(ts)
                    # chart MCP å¯èƒ½éœ€è¦ç®€å•çš„æ ¼å¼ï¼Œå¦‚ "HH:MM" æˆ– "YYYY-MM-DD HH:MM:SS"
                    time_strings.append(dt.strftime('%Y-%m-%d %H:%M:%S'))
                else:
                    time_strings.append(str(ts))
            
            # å‡†å¤‡æŠ˜çº¿å›¾æ•°æ®
            chart_data = []
            for time_str, value in zip(time_strings, values):
                chart_data.append({
                    "time": time_str,
                    "value": float(value)
                })
            
            # æ„å»ºå›¾è¡¨æ ‡é¢˜
            title = f"{metric_name} - {resource_identifier}"
            
            # ç¡®å®š Y è½´æ ‡é¢˜
            unit = '%' if 'usage' in metric_name.lower() or 'pct' in metric_name.lower() else ''
            y_axis_title = f"{metric_name} ({unit})" if unit else metric_name
            
            # è°ƒç”¨ chart MCP å·¥å…·ç”ŸæˆæŠ˜çº¿å›¾
            chart_result = self.mcp_client.call_tool(
                "mcp-server-chart",
                "generate_line_chart",
                {
                    "data": chart_data,
                    "title": title,
                    "axisXTitle": "æ—¶é—´",
                    "axisYTitle": y_axis_title,
                    "width": 1200,
                    "height": 600,
                    "theme": "default"
                }
            )
            
            # chart MCP è¿”å›çš„æ˜¯ base64 ç¼–ç çš„å›¾ç‰‡æ•°æ®æˆ–æ–‡ä»¶è·¯å¾„
            # éœ€è¦ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
            if chart_result:
                # ç”Ÿæˆæ–‡ä»¶åï¼ˆæ¸…ç†ç‰¹æ®Šå­—ç¬¦ï¼‰
                safe_identifier = "".join(c for c in resource_identifier if c.isalnum() or c in ('-', '_', '.'))
                safe_metric = "".join(c for c in metric_name if c.isalnum() or c in ('-', '_'))
                filename = f"{safe_identifier}_{safe_metric}.png"
                filepath = self.metrics_output_dir / filename
                
                # ä½¿ç”¨ç»Ÿä¸€çš„ä¿å­˜æ–¹æ³•
                self._save_chart_result(chart_result, filepath)
                
                return str(filepath)
            
            return None
        except Exception as e:
            print(f"é”™è¯¯: ç”Ÿæˆå›¾è¡¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def generate_error_statistics_charts(
        self,
        error_levels: Dict[str, int],
        error_types: Optional[Dict[str, int]] = None,
        output_prefix: str = "error"
    ) -> Dict[str, str]:
        """
        ç”Ÿæˆé”™è¯¯ç»Ÿè®¡å›¾è¡¨ï¼ˆä½¿ç”¨ chart MCP å·¥å…·ï¼‰
        
        Args:
            error_levels: é”™è¯¯çº§åˆ«ç»Ÿè®¡ï¼Œå¦‚ {"CRITICAL": 30, "ERROR": 40, "WARNING": 50}
            error_types: é”™è¯¯ç±»å‹ç»Ÿè®¡ï¼Œå¦‚ {"æ”¯ä»˜ç³»ç»Ÿä¸å¯ç”¨": 10, "æ•°æ®åº“è¿æ¥å¤±è´¥": 20}
            output_prefix: è¾“å‡ºæ–‡ä»¶å‰ç¼€ï¼Œé»˜è®¤ "error"
            
        Returns:
            ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶è·¯å¾„å­—å…¸ï¼Œå¦‚ {"levels": "path/to/error_levels.png", "types": "path/to/error_types.png"}
        """
        chart_paths = {}
        
        if not self.mcp_client:
            print("è­¦å‘Š: MCP å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œæ— æ³•ç”Ÿæˆå›¾è¡¨")
            return chart_paths
        
        # ç”Ÿæˆé”™è¯¯çº§åˆ«åˆ†å¸ƒé¥¼å›¾
        if error_levels:
            try:
                # å‡†å¤‡é¥¼å›¾æ•°æ®
                pie_data = []
                for category, value in error_levels.items():
                    pie_data.append({
                        "category": category,
                        "value": value
                    })
                
                # è°ƒç”¨ chart MCP å·¥å…·ç”Ÿæˆé¥¼å›¾
                chart_result = self.mcp_client.call_tool(
                    "mcp-server-chart",
                    "generate_pie_chart",
                    {
                        "data": pie_data,
                        "title": "é”™è¯¯çº§åˆ«åˆ†å¸ƒ",
                        "width": 800,
                        "height": 800,
                        "theme": "default"
                    }
                )
                
                if chart_result:
                    output_path = self.metrics_output_dir / f"{output_prefix}_levels.png"
                    self._save_chart_result(chart_result, output_path)
                    chart_paths["levels"] = str(output_path)
                    print(f"âœ… å·²ç”Ÿæˆé”™è¯¯çº§åˆ«å›¾è¡¨: {output_path}")
            except Exception as e:
                print(f"âŒ ç”Ÿæˆé”™è¯¯çº§åˆ«å›¾è¡¨å¤±è´¥: {e}")
        
        # ç”Ÿæˆé”™è¯¯ç±»å‹åˆ†å¸ƒæŸ±çŠ¶å›¾
        if error_types:
            try:
                # æŒ‰å‡ºç°æ¬¡æ•°æ’åº
                sorted_types = sorted(error_types.items(), key=lambda x: x[1], reverse=True)
                
                # å‡†å¤‡æŸ±çŠ¶å›¾æ•°æ®
                bar_data = []
                for category, value in sorted_types:
                    bar_data.append({
                        "category": category,
                        "value": value
                    })
                
                # è°ƒç”¨ chart MCP å·¥å…·ç”ŸæˆæŸ±çŠ¶å›¾
                chart_result = self.mcp_client.call_tool(
                    "mcp-server-chart",
                    "generate_bar_chart",
                    {
                        "data": bar_data,
                        "title": "é”™è¯¯ç±»å‹åˆ†å¸ƒ",
                        "axisXTitle": "å‡ºç°æ¬¡æ•°",
                        "axisYTitle": "é”™è¯¯ç±»å‹",
                        "width": 1200,
                        "height": 800,
                        "theme": "default"
                    }
                )
                
                if chart_result:
                    output_path = self.metrics_output_dir / f"{output_prefix}_types.png"
                    self._save_chart_result(chart_result, output_path)
                    chart_paths["types"] = str(output_path)
                    print(f"âœ… å·²ç”Ÿæˆé”™è¯¯ç±»å‹å›¾è¡¨: {output_path}")
            except Exception as e:
                print(f"âŒ ç”Ÿæˆé”™è¯¯ç±»å‹å›¾è¡¨å¤±è´¥: {e}")
        
        return chart_paths
    
    def _save_chart_result(self, chart_result: Any, filepath: Path) -> None:
        """
        ä¿å­˜å›¾è¡¨ç»“æœåˆ°æ–‡ä»¶
        
        Args:
            chart_result: chart MCP å·¥å…·è¿”å›çš„ç»“æœï¼ˆå¯èƒ½æ˜¯ base64 å­—ç¬¦ä¸²ã€æ–‡ä»¶è·¯å¾„æˆ–å…¶ä»–æ ¼å¼ï¼‰
            filepath: ä¿å­˜è·¯å¾„
        """
        try:
            import base64
            import shutil
            
            # å¦‚æœè¿”å›çš„æ˜¯ base64 æ•°æ®ï¼Œéœ€è¦è§£ç ä¿å­˜
            if isinstance(chart_result, dict):
                # å¯èƒ½æ˜¯ {"image": "base64..."} æ ¼å¼
                if "image" in chart_result:
                    image_data = chart_result["image"]
                    if isinstance(image_data, str):
                        if image_data.startswith("data:image"):
                            # ç§»é™¤ data:image/png;base64, å‰ç¼€
                            image_data = image_data.split(",", 1)[1]
                        with open(filepath, "wb") as f:
                            f.write(base64.b64decode(image_data))
                        return
                # å¯èƒ½æ˜¯ {"file": "path/to/file"} æ ¼å¼
                if "file" in chart_result:
                    shutil.copy(chart_result["file"], filepath)
                    return
            elif isinstance(chart_result, str):
                # å¦‚æœè¿”å›çš„æ˜¯ base64 å­—ç¬¦ä¸²
                if chart_result.startswith("data:image"):
                    image_data = chart_result.split(",", 1)[1]
                    with open(filepath, "wb") as f:
                        f.write(base64.b64decode(image_data))
                    return
                # å¦‚æœè¿”å›çš„æ˜¯æ–‡ä»¶è·¯å¾„
                if Path(chart_result).exists():
                    shutil.copy(chart_result, filepath)
                    return
            elif isinstance(chart_result, bytes):
                # ç›´æ¥æ˜¯äºŒè¿›åˆ¶æ•°æ®
                with open(filepath, "wb") as f:
                    f.write(chart_result)
                return
            
            # å…¶ä»–æ ¼å¼ï¼Œå°è¯•ç›´æ¥å†™å…¥
            with open(filepath, "wb") as f:
                if isinstance(chart_result, bytes):
                    f.write(chart_result)
                else:
                    f.write(str(chart_result).encode())
        except Exception as e:
            print(f"é”™è¯¯: ä¿å­˜å›¾è¡¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def format_output(self, analysis_result: Dict[str, Any], format_type: str = "text", auto_query_metrics: bool = True) -> str:
        """
        æ ¼å¼åŒ–è¾“å‡ºç»“æœï¼ˆè‡ªåŠ¨æŸ¥è¯¢æŒ‡æ ‡å¹¶ç”Ÿæˆå›¾è¡¨ï¼‰
        
        Args:
            analysis_result: åˆ†æç»“æœå­—å…¸
            format_type: è¾“å‡ºæ ¼å¼ï¼Œæ”¯æŒ "text", "json", "markdown"
            auto_query_metrics: æ˜¯å¦è‡ªåŠ¨æŸ¥è¯¢æŒ‡æ ‡ï¼ˆå¦‚æœç»“æœä¸­æ²¡æœ‰æŒ‡æ ‡æ•°æ®ï¼‰ï¼Œé»˜è®¤ True
            
        Returns:
            æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
        """
        # å¦‚æœæ˜¯ Markdown æ ¼å¼ä¸”å¯ç”¨äº†è‡ªåŠ¨æŸ¥è¯¢æŒ‡æ ‡ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æŸ¥è¯¢æŒ‡æ ‡
        if format_type == "markdown" and auto_query_metrics and self.enable_metrics_query and self.mcp_client:
            config = analysis_result.get("analysis_config", {})
            metrics = analysis_result.get("metrics", {})
            
            # å¦‚æœæ²¡æœ‰æŒ‡æ ‡æ•°æ®ï¼Œå°è¯•è‡ªåŠ¨æŸ¥è¯¢
            if not metrics or not metrics.get("resources"):
                filter_fields = config.get("filter_fields", {})
                results = analysis_result.get("results", {})
                time_range = config.get("time_range", {})
                start_time = time_range.get("start_time")
                end_time = time_range.get("end_time")
                bk_biz_id = config.get("bk_biz_id")
                
                if start_time and end_time and bk_biz_id:
                    # è¯†åˆ«èµ„æºå¹¶æŸ¥è¯¢æŒ‡æ ‡
                    metrics_results = self._query_related_metrics(
                        bk_biz_id, filter_fields, results, start_time, end_time
                    )
                    if metrics_results and metrics_results.get("resources"):
                        analysis_result["metrics"] = metrics_results
        
        if format_type == "json":
            return json.dumps(analysis_result, ensure_ascii=False, indent=2)
        
        elif format_type == "markdown":
            return self._format_markdown(analysis_result)
        
        else:  # text
            return self._format_text(analysis_result)
    
    def _format_text(self, result: Dict[str, Any]) -> str:
        """æ–‡æœ¬æ ¼å¼è¾“å‡º"""
        lines = []
        config = result.get("analysis_config", {})
        results = result.get("results", {})
        summary = result.get("summary", {})
        
        lines.append("=" * 80)
        lines.append("æ—¥å¿—å¤šç»´åº¦åˆ†ææŠ¥å‘Š")
        lines.append("=" * 80)
        lines.append("")
        
        # é…ç½®ä¿¡æ¯
        lines.append("åˆ†æé…ç½®:")
        lines.append(f"  ä¸šåŠ¡ID: {config.get('bk_biz_id')}")
        lines.append(f"  ç´¢å¼•é›†ID: {config.get('index_set_id')}")
        lines.append(f"  è¿‡æ»¤æ¡ä»¶: {config.get('filter_fields')}")
        lines.append(f"  åˆ†ç»„å­—æ®µ: {config.get('group_by_field')}")
        lines.append(f"  æ‹†åˆ†å­—æ®µ: {config.get('split_by_field')}")
        time_range = config.get("time_range", {})
        lines.append(f"  æ—¶é—´èŒƒå›´: {time_range.get('start_time_str')} ~ {time_range.get('end_time_str')}")
        lines.append("")
        
        # æ±‡æ€»ä¿¡æ¯
        lines.append("æ±‡æ€»ç»Ÿè®¡:")
        lines.append(f"  æ€»åˆ†ç»„æ•°: {summary.get('total_groups', 0)}")
        lines.append(f"  æ€»æ—¥å¿—æ•°: {summary.get('overall_total', 0)}")
        lines.append("")
        
        # æ‹†åˆ†å­—æ®µæ±‡æ€»
        split_summary = summary.get("split_field_summary", {})
        if split_summary:
            lines.append(f"  æŒ‰ {config.get('split_by_field')} æ±‡æ€»:")
            for split_value, count in sorted(split_summary.items(), key=lambda x: x[1], reverse=True):
                lines.append(f"    {split_value}: {count}")
            lines.append("")
        
        # è¯¦ç»†ç»“æœ
        lines.append("è¯¦ç»†åˆ†æç»“æœ:")
        lines.append("-" * 80)
        for group_value, group_data in sorted(results.items()):
            lines.append(f"\n[{config.get('group_by_field')}: {group_value}]")
            lines.append(f"  æ€»æ—¥å¿—æ•°: {group_data.get('total', 0)}")
            
            distribution = group_data.get("distribution", {})
            if distribution:
                lines.append(f"  æŒ‰ {config.get('split_by_field')} åˆ†å¸ƒ:")
                for split_value, count in sorted(distribution.items(), key=lambda x: x[1], reverse=True):
                    percentage = (count / group_data.get('total', 1)) * 100 if group_data.get('total', 0) > 0 else 0
                    lines.append(f"    {split_value}: {count} ({percentage:.1f}%)")
        
        lines.append("")
        lines.append("=" * 80)
        
        return "\n".join(lines)
    
    def _format_markdown(self, result: Dict[str, Any]) -> str:
        """Markdown æ ¼å¼è¾“å‡ºï¼ˆä¼˜åŒ–ç‰ˆï¼šç»“æ„æ¸…æ™°ï¼Œé¿å…é‡å¤ï¼‰"""
        lines = []
        config = result.get("analysis_config", {})
        results = result.get("results", {})
        summary = result.get("summary", {})
        
        # æ ‡é¢˜
        lines.append("# æ—¥å¿—åˆ†ææŠ¥å‘Š\n")
        
        # åŸºæœ¬ä¿¡æ¯ï¼ˆç®€åŒ–ï¼‰
        time_range = config.get("time_range", {})
        lines.append(f"**æ—¶é—´èŒƒå›´**: {time_range.get('start_time_str')} ~ {time_range.get('end_time_str')}  ")
        lines.append(f"**æ€»æ—¥å¿—æ•°**: {summary.get('overall_total', 0)}  ")
        lines.append(f"**åˆ†æç»´åº¦**: {summary.get('total_groups', 0)} ä¸ª {config.get('group_by_field', '')}\n")
        
        # å…³é”®ç»Ÿè®¡ï¼ˆåˆå¹¶æ˜¾ç¤ºï¼Œé¿å…é‡å¤ï¼‰
        split_summary = summary.get("split_field_summary", {})
        if split_summary:
            lines.append("## ç»Ÿè®¡æ¦‚è§ˆ\n")
            lines.append("| ç±»å‹ | æ•°é‡ | å æ¯” |")
            lines.append("|---|---|---|")
            total = summary.get('overall_total', 1)
            for split_value, count in sorted(split_summary.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total) * 100 if total > 0 else 0
                lines.append(f"| {split_value} | {count} | {percentage:.1f}% |")
            lines.append("")
        
        # é”™è¯¯ç±»å‹åˆ†å¸ƒå›¾è¡¨ï¼ˆä»…å½“åˆ†ç»„å­—æ®µæ˜¯ä»£ç æ–‡ä»¶/æ¨¡å—æ—¶ç”Ÿæˆï¼‰
        error_chart_paths = {}
        group_by_field = config.get('group_by_field', '').lower()
        split_by_field = config.get('split_by_field', '').lower()
        
        if group_by_field in ['code_file', 'file_name', 'module', 'service', 'component']:
            # åªç”Ÿæˆé”™è¯¯ç±»å‹åˆ†å¸ƒå›¾è¡¨ï¼ˆä¸ç”Ÿæˆé”™è¯¯çº§åˆ«é¥¼å›¾ï¼‰
            error_types = {}
            for group_value, group_data in results.items():
                # åªç»Ÿè®¡ä¸¥é‡é”™è¯¯ï¼ˆCRITICAL + ERRORï¼‰
                if split_by_field in ['level', 'log_level', 'severity', 'error_level']:
                    distribution = group_data.get('distribution', {})
                    error_count = 0
                    for level, count in distribution.items():
                        level_upper = str(level).upper()
                        if level_upper in ['CRITICAL', 'ERROR']:
                            error_count += count
                    if error_count > 0:
                        error_types[str(group_value)] = error_count
            
            if error_types:
                chart_paths = self.generate_error_statistics_charts(
                    error_levels=None,
                    error_types=error_types,
                    output_prefix="error_types"
                )
                if chart_paths.get("types"):
                    error_chart_paths["types"] = chart_paths["types"]
        
        # åµŒå…¥é”™è¯¯ç±»å‹åˆ†å¸ƒå›¾è¡¨
        if error_chart_paths.get("types"):
            lines.append("## é”™è¯¯åˆ†å¸ƒ\n")
            chart_path = error_chart_paths["types"]
            chart_filename = os.path.basename(chart_path)
            try:
                rel_path = os.path.relpath(chart_path)
            except ValueError:
                rel_path = f"{self.metrics_output_dir.name}/{chart_filename}"
            lines.append(f"![é”™è¯¯ç±»å‹åˆ†å¸ƒ]({rel_path})\n")
            lines.append("")
        
        # è¯¦ç»†ç»“æœï¼ˆç®€åŒ–è¡¨æ ¼ï¼Œé¿å…é‡å¤ä¿¡æ¯ï¼‰
        if results:
            lines.append("## è¯¦ç»†åˆ†æ\n")
            
            # è·å–æ‰€æœ‰æ‹†åˆ†å­—æ®µçš„å€¼ä½œä¸ºè¡¨å¤´
            all_split_values = set()
            for group_data in results.values():
                distribution = group_data.get("distribution", {})
                all_split_values.update(distribution.keys())
            
            if all_split_values:
                # æ„å»ºè¡¨å¤´
                sorted_split_values = sorted(all_split_values)
                header = f"| {config.get('group_by_field', 'åˆ†ç»„')} | æ€»æ•° | "
                header += " | ".join(sorted_split_values) + " |\n"
                lines.append(header)
                
                # è¡¨å¤´åˆ†éš”çº¿
                header_sep = "|" + "|".join(["---"] * (2 + len(sorted_split_values))) + "|\n"
                lines.append(header_sep)
                
                # æ•°æ®è¡Œ
                for group_value, group_data in sorted(results.items(), key=lambda x: x[1].get('total', 0), reverse=True):
                    total = group_data.get('total', 0)
                    distribution = group_data.get("distribution", {})
                    row = f"| {group_value} | {total} | "
                    row += " | ".join([str(distribution.get(split_value, 0)) for split_value in sorted_split_values]) + " |\n"
                    lines.append(row)
            else:
                # å¦‚æœæ²¡æœ‰æ‹†åˆ†å­—æ®µï¼Œåªæ˜¾ç¤ºæ€»æ•°
                lines.append("| åˆ†ç»„ | æ€»æ•° |\n")
                lines.append("|---|---|\n")
                for group_value, group_data in sorted(results.items(), key=lambda x: x[1].get('total', 0), reverse=True):
                    lines.append(f"| {group_value} | {group_data.get('total', 0)} |\n")
            lines.append("")
        
        # æŒ‡æ ‡åˆ†æéƒ¨åˆ†ï¼ˆè‡ªåŠ¨ç”Ÿæˆæ—¶åºå›¾è¡¨ï¼Œç®€åŒ–æ˜¾ç¤ºï¼‰
        metrics = result.get("metrics", {})
        if metrics and metrics.get("resources"):
            lines.append("## ğŸ–¥ï¸ è®¾å¤‡èµ„æºç›‘æ§åˆ†æ\n")
            
            for resource_info in metrics["resources"]:
                resource_type = resource_info["resource_type"]
                resource_value = resource_info["resource_value"]
                resource_metrics = resource_info["metrics"]
                
                lines.append(f"### {resource_type}: {resource_value}\n")
                
                # åˆå¹¶æ˜¾ç¤ºæ‰€æœ‰æŒ‡æ ‡çš„ç»Ÿè®¡ä¿¡æ¯
                if resource_metrics:
                    lines.append("| æŒ‡æ ‡ | å¹³å‡å€¼ | æœ€å¤§å€¼ | æœ€å°å€¼ |\n")
                    lines.append("|---|---|---|---|\n")
                    
                    chart_paths = []
                    for metric_name, metric_data in resource_metrics.items():
                        # ç¡®ä¿å›¾è¡¨å·²ç”Ÿæˆ
                        chart_path = metric_data.get("chart_path")
                        if not chart_path and metric_data.get("timestamps") and metric_data.get("values"):
                            config = result.get("analysis_config", {})
                            time_range = config.get("time_range", {})
                            start_time = time_range.get("start_time")
                            end_time = time_range.get("end_time")
                            
                            if start_time and end_time:
                                chart_path = self._generate_metric_chart(
                                    metric_data,
                                    f"{resource_type}_{resource_value}_{metric_name}",
                                    f"{metric_name}",
                                    start_time,
                                    end_time
                                )
                                if chart_path:
                                    metric_data["chart_path"] = chart_path
                        
                        stats = metric_data.get("statistics", {})
                        if stats:
                            lines.append(f"| {metric_name} | {stats.get('avg', 0):.2f} | {stats.get('max', 0):.2f} | {stats.get('min', 0):.2f} |\n")
                        
                        if chart_path:
                            chart_paths.append((metric_name, chart_path))
                    
                    lines.append("")
                    
                    # æ˜¾ç¤ºæ‰€æœ‰å›¾è¡¨
                    for metric_name, chart_path in chart_paths:
                        chart_filename = os.path.basename(chart_path)
                        try:
                            rel_path = os.path.relpath(chart_path)
                        except ValueError:
                            rel_path = f"{self.metrics_output_dir.name}/{chart_filename}"
                        lines.append(f"#### {metric_name}\n")
                        lines.append(f"![{metric_name}]({rel_path})\n")
                        lines.append("")
        
        # å¦‚æœ metrics ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œå°è¯•å›é€€æŸ¥è¯¢ä¸»æœºæŒ‡æ ‡
        if (not metrics or not metrics.get("resources")) and self.enable_metrics_query and self.mcp_client:
            # å³ä½¿æ²¡æœ‰è¯†åˆ«åˆ°èµ„æºï¼Œä¹Ÿå°è¯•ä» filter_fields ä¸­æŸ¥è¯¢é€šç”¨æŒ‡æ ‡
            config = result.get("analysis_config", {})
            filter_fields = config.get("filter_fields", {})
            time_range = config.get("time_range", {})
            start_time = time_range.get("start_time")
            end_time = time_range.get("end_time")
            bk_biz_id = config.get("bk_biz_id")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸»æœºIPï¼Œå¦‚æœæœ‰åˆ™æŸ¥è¯¢ä¸»æœºæŒ‡æ ‡
            host_ip = None
            for field, value in filter_fields.items():
                if field.lower() in ['serverip', 'ip', 'host']:
                    host_ip = str(value) if not isinstance(value, list) else str(value[0]) if value else None
                    break
            
            # å¦‚æœæ²¡æœ‰ä» filter_fields ä¸­æ‰¾åˆ°ï¼Œå°è¯•ä»ç»“æœä¸­æŸ¥æ‰¾IPåœ°å€
            if not host_ip:
                results = result.get("results", {})
                print(f"ğŸ” ä»ç»“æœä¸­æŸ¥æ‰¾IPåœ°å€ï¼Œç»“æœæ•°é‡: {len(results)}")
                for group_value in results.keys():
                    if self._is_ip_address(str(group_value)):
                        host_ip = str(group_value)
                        print(f"âœ… ä»ç»“æœä¸­è¯†åˆ«åˆ°ä¸»æœºIP: {host_ip}")
                        break
            
            if host_ip and start_time and end_time and bk_biz_id:
                print(f"ğŸ” å‡†å¤‡æŸ¥è¯¢ä¸»æœºæŒ‡æ ‡: {host_ip}, ä¸šåŠ¡ID: {bk_biz_id}, æ—¶é—´èŒƒå›´: {start_time} ~ {end_time}")
                lines.append("## ğŸ–¥ï¸ è®¾å¤‡èµ„æºç›‘æ§åˆ†æ\n")
                lines.append(f"### ä¸»æœº: {host_ip}\n")
                
                # æŸ¥è¯¢ä¸»æœºé€šç”¨æŒ‡æ ‡
                host_metrics = self._query_host_metrics(bk_biz_id, host_ip, start_time, end_time)
                
                if host_metrics:
                    # åˆå¹¶æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                    lines.append("| æŒ‡æ ‡ | å¹³å‡å€¼ | æœ€å¤§å€¼ | æœ€å°å€¼ |\n")
                    lines.append("|---|---|---|---|\n")
                    
                    chart_paths = []
                    for metric_name, metric_data in host_metrics.items():
                        # ç”Ÿæˆå›¾è¡¨
                        chart_path = self._generate_metric_chart(
                            metric_data,
                            f"host_{host_ip}_{metric_name}",
                            f"{metric_name}",
                            start_time,
                            end_time
                        )
                        
                        stats = metric_data.get("statistics", {})
                        if stats:
                            lines.append(f"| {metric_name} | {stats.get('avg', 0):.2f} | {stats.get('max', 0):.2f} | {stats.get('min', 0):.2f} |\n")
                        
                        if chart_path:
                            chart_paths.append((metric_name, chart_path))
                    
                    lines.append("")
                    
                    # æ˜¾ç¤ºæ‰€æœ‰å›¾è¡¨
                    for metric_name, chart_path in chart_paths:
                        chart_filename = os.path.basename(chart_path)
                        try:
                            rel_path = os.path.relpath(chart_path)
                        except ValueError:
                            rel_path = f"{self.metrics_output_dir.name}/{chart_filename}"
                        lines.append(f"#### {metric_name}\n")
                        lines.append(f"![{metric_name}]({rel_path})\n")
                        lines.append("")
        
        return "\n".join(lines)
    
    def _query_host_metrics(self, bk_biz_id: str, host_ip: str, start_time: int, end_time: int) -> Dict[str, Dict[str, Any]]:
        """
        æŸ¥è¯¢ä¸»æœºé€šç”¨æŒ‡æ ‡ï¼ˆCPUã€å†…å­˜ã€ç£ç›˜ç­‰ï¼‰
        
        Args:
            bk_biz_id: ä¸šåŠ¡ID
            host_ip: ä¸»æœºIP
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            
        Returns:
            æŒ‡æ ‡æ•°æ®å­—å…¸
        """
        if not self.mcp_client:
            return {}
        
        host_metrics = {}
        
        # ä¸»æœºé€šç”¨æŒ‡æ ‡é…ç½®
        host_metric_configs = {
            "CPUä½¿ç”¨ç‡": "avg(avg_over_time(bkmonitor:system:cpu_summary:usage{ip=\"%s\"}[1m]))",
            "å†…å­˜ä½¿ç”¨ç‡": "avg(avg_over_time(bkmonitor:system:mem:pct_used{ip=\"%s\"}[1m]))",
            "ç£ç›˜ä½¿ç”¨ç‡": "avg(avg_over_time(bkmonitor:system:disk:in_use{ip=\"%s\"}[1m]))",
            "ç£ç›˜IOä½¿ç”¨ç‡": "avg(avg_over_time(bkmonitor:system:io:util{ip=\"%s\"}[1m]))",
            "ç³»ç»Ÿè´Ÿè½½": "avg(avg_over_time(bkmonitor:system:system:load:load5{ip=\"%s\"}[5m]))"
        }
        
        print(f"ğŸ” å¼€å§‹æŸ¥è¯¢ä¸»æœºæŒ‡æ ‡: {host_ip}, æ—¶é—´èŒƒå›´: {start_time} ~ {end_time}")
        
        for metric_name, promql_template in host_metric_configs.items():
            try:
                promql = promql_template % host_ip
                print(f"  ğŸ“Š æŸ¥è¯¢æŒ‡æ ‡: {metric_name}, PromQL: {promql}")
                
                query_result = self.mcp_client.call_tool(
                    "bkmonitor-metrics-bkop",
                    "execute_range_query",
                    {
                        "body_param": {
                            "bk_biz_id": bk_biz_id,
                            "promql": promql,
                            "start_time": str(start_time),
                            "end_time": str(end_time),
                            "step": "1m"
                        }
                    }
                )
                
                metric_data = self._parse_metric_result(query_result)
                if metric_data:
                    host_metrics[metric_name] = metric_data
                    print(f"  âœ… æˆåŠŸè·å–æŒ‡æ ‡: {metric_name}, æ•°æ®ç‚¹æ•°: {len(metric_data.get('values', []))}")
                else:
                    print(f"  âš ï¸ æŒ‡æ ‡æ•°æ®ä¸ºç©º: {metric_name}")
            except Exception as e:
                print(f"  âŒ æŸ¥è¯¢ä¸»æœºæŒ‡æ ‡å¤±è´¥ {host_ip}, metric={metric_name}: {e}")
                import traceback
                traceback.print_exc()
        
        print(f"ğŸ“ˆ ä¸»æœºæŒ‡æ ‡æŸ¥è¯¢å®Œæˆï¼Œå…±è·å– {len(host_metrics)} ä¸ªæŒ‡æ ‡")
        
        return host_metrics


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python log_multi_dimensional_analyzer.py <config_json_file>")
        print("\né…ç½®æ–‡ä»¶æ ¼å¼:")
        print(json.dumps({
            "bk_biz_id": "2",
            "index_set_id": "322",
            "filter_fields": {"namespace": "xxx", "svr": "yyy"},
            "group_by_field": "file_name",
            "split_by_field": "level",
            "start_time": 1702300000,
            "end_time": 1702386400,
            "query_string": "*"
        }, indent=2, ensure_ascii=False))
        sys.exit(1)
    
    # è¯»å–é…ç½®
    config_file = sys.argv[1]
    with open(config_file, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    # åˆ›å»ºåˆ†æå™¨ï¼ˆä¸ä½¿ç”¨ MCP å®¢æˆ·ç«¯ï¼Œç›´æ¥ä½¿ç”¨ç»“æœï¼‰
    analyzer = LogMultiDimensionalAnalyzer()
    
    # æ‰§è¡Œåˆ†æï¼ˆéœ€è¦æ‰‹åŠ¨è°ƒç”¨ MCP å·¥å…·ï¼‰
    print("æ³¨æ„: æ­¤è„šæœ¬éœ€è¦é…åˆ MCP å·¥å…·ä½¿ç”¨")
    print("è¯·ä½¿ç”¨ MCP å·¥å…·è°ƒç”¨ analyze_field è¿›è¡Œå®é™…åˆ†æ")


if __name__ == "__main__":
    main()

